"""
Handles generating learnset codetables which are packed into NARC subfiles via codetables.mk

Responsible for generating the following:
  data/generated/MachineMoveLearnsets.c
  data/generated/LevelupLearnsets.c
  data/generated/EggLearnsets.c
"""

import re
import json
import os
import argparse
import glob


def load_species_header(file_path):
    species_dict = {}
    index = 0
    with open(file_path) as f:
        for line in f:
            if len(line.split()) > 1:
                test = line.split()[1].strip()
                if 'SPECIES' in test and not '_START' in test and not '_SPECIES_H' in test and not '_NUM (' in line and not 'MAX_' in test:
                    species_dict[test] = index
                    index += 1
    return species_dict


def load_moves_header(file_path):
    moves_dict = {}
    index = 0
    with open(file_path) as f:
        for line in f:
            if len(line.split()) > 1:
                test = line.split()[1].strip()
                if 'MOVE' in test and not '_START' in test and not '_MOVES_H' in test and not 'NUM_OF' in test:
                    moves_dict[test] = index
                    index += 1
    return moves_dict


def load_machine_move_list(file_path):
    move_list = []
    in_array = False
    move_pattern = re.compile(r'\bMOVE_[A-Z0-9_]+')

    with open(file_path, encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if 'static const u16 sMachineMoves[]' in line:
                in_array = True
                continue
            if in_array:
                if '};' in line:
                    break
                matches = move_pattern.findall(line)
                move_list.extend(matches)

    return move_list


def load_form_to_species_mapping(form_map_path):
    form_map = {}

    define_pattern = re.compile(r"\[(SPECIES_\w+)\s*-\s*SPECIES_MEGA_START\]\s*=\s*(SPECIES_\w+),")

    with open(form_map_path) as f:
        for line in f:
            match = define_pattern.search(line)
            if match:
                form_species, base_species = match.groups()
                form_map[form_species] = base_species

    return form_map


def load_config_header(config_path):
    config = {}
    define_pattern = re.compile(r"#define\s+(\w+)(\s+\"?.*?\"?)?$")

    with open(config_path, "r") as f:
        for line in f:
            match = define_pattern.match(line.strip())
            if match:
                key = match.group(1)
                val = match.group(2).strip() if match.group(2) else True

                if isinstance(val, str) and val.startswith('"') and val.endswith('"'):
                    val = val[1:-1]

                config[key] = val

    return config


def merge_learnsets(ordered_data, cutoff_gen, inherit_level, inherit_egg, inherit_machine, inherit_tutor):
    merged = {}

    for gen_file, gen_data in ordered_data:
        gen_key = os.path.basename(gen_file)[:-5]  # remove .json extension
        if gen_key != "custom":
            gen_key = gen_key[3:]  # remove XX_ prefix

        for species, fields in gen_data.items():
            merged.setdefault(species, {
                "LevelMoves": [],
                "MachineMoves": set(),
                "EggMoves": [],
                "TutorMoves": set()
            })

            if fields.get("LevelMoves") and (gen_key == cutoff_gen or inherit_level):
                merged[species]["LevelMoves"] = fields.get("LevelMoves", [])

            if fields.get("EggMoves") and (gen_key == cutoff_gen or inherit_level):
                merged[species]["EggMoves"] = fields.get("EggMoves", [])

            if gen_key == cutoff_gen or inherit_machine:
                merged[species]["MachineMoves"].update(fields.get("MachineMoves", []))

            if gen_key == cutoff_gen or inherit_tutor:
                merged[species]["TutorMoves"].update(fields.get("TutorMoves", []))

        if gen_key == cutoff_gen:
            break

    for data in merged.values():
        data["MachineMoves"] = sorted(data["MachineMoves"])
        data["TutorMoves"] = sorted(data["TutorMoves"])

    return merged


def write_learnset_constants_inc(max_num_levelup_moves, output_path):
    os.makedirs(os.path.dirname(output_path), exist_ok=True)

    with open(output_path, "w") as f:
        f.write("// DO NOT MODIFY THIS FILE! Autogenerated by build_learnsets.py\n\n")
        f.write(f"MAX_LEVELUP_MOVES equ {max_num_levelup_moves}")


def write_learnset_constants_header(num_machine_moves, max_num_levelup_moves, max_num_egg_moves, output_path):
    os.makedirs(os.path.dirname(output_path), exist_ok=True)

    with open(output_path, "w") as f:
        f.write("// DO NOT MODIFY THIS FILE! Autogenerated by build_learnsets.py\n\n")
        f.write("#ifndef GENERATED_LEARNSET_CONSTANTS_H\n")
        f.write("#define GENERATED_LEARNSET_CONSTANTS_H\n\n")
        f.write(f"#define NUM_MACHINE_MOVES {num_machine_moves}\n")
        f.write(f"#define MAX_LEVELUP_MOVES {max_num_levelup_moves}\n")
        f.write(f"#define MAX_EGG_MOVES     {max_num_egg_moves}\n\n")
        f.write(f"#define MACHINE_LEARNSETS_BITFIELD_COUNT ((NUM_MACHINE_MOVES + 31) / 32)\n\n")
        f.write("#endif // GENERATED_LEARNSET_CONSTANTS_H\n")


def write_machine_data(species_dict, species_learnsets, machine_moves, levelup_tm_threshold, output_path):
    max_species_index = max(species_dict.values())
    species_id_to_name = {v: k for k, v in species_dict.items()}
    if levelup_tm_threshold is not None and type(levelup_tm_threshold) is str:
        levelup_tm_threshold = int(levelup_tm_threshold)

    with open(output_path, "w") as out:
        out.write("// DO NOT MODIFY THIS FILE!  autogenerated by build_learnsets.py\n\n")
        out.write("#include \"../../include/types.h\"\n")
        out.write("#include \"../../include/constants/species.h\"\n")
        out.write("#include \"../../include/constants/generated/learnsets.h\"\n\n")
        out.write(f"const u32 UNUSED MachineMoveLearnsets[][MACHINE_LEARNSETS_BITFIELD_COUNT] = {{\n")

        for species_id in range(max_species_index + 1):
            species_name = species_id_to_name.get(species_id)
            learnset = []
            levelup_moves = {}
            if species_name:
                learnset = species_learnsets.get(species_name, {}).get("MachineMoves", [])
                learnset = list(set(m.strip() for m in learnset))

                levelup_moves = {
                    m["Move"] for m in species_learnsets.get(species_name, {}).get("LevelMoves", [])
                    if "Move" in m
                }

            parts = [0] * ((len(machine_moves) + 31) // 32)
            for i, move in enumerate(machine_moves):
                if move in learnset:
                    move_index = i
                elif levelup_tm_threshold is not None and (i + 1) > levelup_tm_threshold and move in levelup_moves:
                    move_index = i
                else:
                    continue

                word = move_index // 32
                bit = move_index % 32
                parts[word] |= (1 << bit)

            formatted = ", ".join(f"0x{val:08X}" for val in parts)
            out.write(f"    [{species_name}] = {{ {formatted} }},\n")

        out.write("};\n")


def write_levelup_data(species_dict, moves_dict, species_learnsets, max_num_levelup_moves, output_path):
    species_id_to_name = {v: k for k, v in species_dict.items()}
    max_species_id = max(species_id_to_name.keys())
    col_len = 8

    with open(output_path, "w") as out:
        out.write("// DO NOT MODIFY THIS FILE! Autogenerated by build_learnsets.py\n\n")
        out.write("#include \"../../include/types.h\"\n")
        out.write("#include \"../../include/constants/generated/learnsets.h\"\n")
        out.write("#include \"../../include/constants/species.h\"\n\n")
        out.write("const u32 UNUSED LevelUpLearnsets[][MAX_LEVELUP_MOVES] = {\n")

        for species_id in range(max_species_id + 1):
            species_name = species_id_to_name.get(species_id)
            learnset = []
            if species_name:
                learnset = species_learnsets.get(species_name, {}).get("LevelMoves", [])

            entries = []

            for move_entry in learnset:
                move = move_entry.get("Move", "").strip()
                level = int(move_entry["Level"])
                if not move or move not in moves_dict:
                    print(f"[ERROR]: Invalid or missing move '{move}' for species '{species_name}' at level {level}")
                    exit(1)

                move_id = moves_dict[move]
                encoded = (level << 16) | move_id
                entries.append(encoded)

            entries.append(0x0000FFFF)
            while len(entries) < max_num_levelup_moves:
                entries.append(0x0000FFFF)

            out.write(f"    [{species_name}] = {{\n")
            for i in range(0, max_num_levelup_moves, col_len):
                line = ", ".join(f"0x{val:08X}" for val in entries[i:i+col_len])
                out.write(f"        {line},\n")
            out.write("    },\n")

        out.write("};\n")


def write_eggmove_data(species_dict, moves_dict, species_learnsets, max_num_egg_moves, output_path):
    species_id_to_name = {v: k for k, v in species_dict.items()}
    max_species_id = max(species_id_to_name)
    col_len = 12

    with open(output_path, "w") as out:
        out.write("// DO NOT MODIFY THIS FILE! Autogenerated by build_learnsets.py\n\n")
        out.write("#include \"../../include/types.h\"\n")
        out.write("#include \"../../include/constants/generated/learnsets.h\"\n")
        out.write("#include \"../../include/constants/species.h\"\n\n")
        out.write("const u16 UNUSED EggMoves[][MAX_EGG_MOVES] = {\n")

        for species_id in range(max_species_id + 1):
            species_name = species_id_to_name.get(species_id, "")
            egg_moves = []

            if species_name:
                egg_moves = species_learnsets.get(species_name, {}).get("EggMoves", [])

            moves = []
            for move in egg_moves:
                if move not in moves_dict:
                    print(f"[ERROR]: Move '{move}' not found in moves.h")
                    exit(1)
                moves.append(moves_dict[move])

            # Add terminator and pad to fixed length
            moves.append(0xFFFF)
            while len(moves) < max_num_egg_moves:
                moves.append(0xFFFF)

            out.write(f"    [{species_name}] = {{\n")
            for i in range(0, max_num_egg_moves, col_len):
                chunk = moves[i:i+col_len]
                out.write("        " + ", ".join(f"0x{m:04X}" for m in chunk) + ",\n")
            out.write("    },\n")

        out.write("};\n")


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--machineout")
    parser.add_argument("--levelupout")
    parser.add_argument("--eggout")
    parser.add_argument("--constsout", action='store_true')
    parser.add_argument("--dump", help="path to dump merged learnsets as JSON")
    args = parser.parse_args()

    config = load_config_header("include/config.h")
    machine_moves = load_machine_move_list("src/item.c")
    species_dict = load_species_header("include/constants/species.h")
    moves_dict = load_moves_header("include/constants/moves.h")
    form_to_base = load_form_to_species_mapping("data/FormToSpeciesMapping.c")

    ordered_learnsets = [
        (file, json.load(open(file, encoding="utf-8")))
        for file in sorted(glob.glob(os.path.join("data/mon/learnsets", "*.json")))
    ]

    species_learnsets = merge_learnsets(
        ordered_learnsets,
        config.get("LEARNSET_FILE", "sv"),
        "LEVELUP_MOVE_INHERITANCE" in config,
        "EGG_MOVE_INHERITANCE" in config,
        "MACHINE_MOVE_INHERITANCE" in config,
        "TUTOR_MOVE_INHERITANCE" in config,
    )

    for form_species, base_species in form_to_base.items():
        if form_species not in species_learnsets and base_species in species_learnsets:
            species_learnsets[form_species] = dict(species_learnsets[base_species])

    max_num_levelup_moves = max(
        (len(data.get("LevelMoves", [])) + 1)  # +1 for terminator
        for data in species_learnsets.values()
    )

    # move reminder limitation
    if max_num_levelup_moves > (256/4):
        print(f"[ERROR]: maximum number of level-up moves cannot exceed 64 ({max_num_levelup_moves})")
        exit(1)

    max_num_egg_moves = max(
        (len(data.get("EggMoves", [])) + 1)  # +1 for terminator
        for data in species_learnsets.values()
    )

    if args.constsout:
        write_learnset_constants_header(len(machine_moves), max_num_levelup_moves, max_num_egg_moves, "include/constants/generated/learnsets.h")
        write_learnset_constants_inc(max_num_levelup_moves, "armips/include/generated/levelup.s")

    if args.machineout:
        write_machine_data(species_dict, species_learnsets, machine_moves, config.get("LEVELUP_MACHINE_MOVE_FALLBACK"), args.machineout)

    if args.levelupout:
        write_levelup_data(species_dict, moves_dict, species_learnsets, max_num_levelup_moves, args.levelupout)

    if args.eggout:
        write_eggmove_data(species_dict, moves_dict, species_learnsets, max_num_egg_moves, args.eggout)

    if args.dump:
        os.makedirs(os.path.dirname(args.dump), exist_ok=True)
        with open(args.dump, "w", encoding="utf-8") as f:
            json.dump(species_learnsets, f, indent=2)
