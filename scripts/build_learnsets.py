"""
Handles generating learnset codetables which are packed into NARC subfiles via codetables.mk

Responsible for generating the following:
  data/generated/EggLearnsets.c
  data/generated/LevelupLearnsets.c
  data/generated/MachineMoveLearnsets.c
  data/generated/TutorMoveLearnsets.c
"""

import re
import json
import os
import argparse
import glob


def load_species_header(file_path):
    species_dict = {}
    index = 0
    with open(file_path) as f:
        for line in f:
            if len(line.split()) > 1:
                test = line.split()[1].strip()
                if 'SPECIES' in test and not '_START' in test and not '_SPECIES_H' in test and not '_NUM (' in line and not 'MAX_' in test:
                    species_dict[test] = index
                    index += 1
    return species_dict


def load_moves_header(file_path):
    moves_dict = {}
    index = 0
    with open(file_path) as f:
        for line in f:
            if len(line.split()) > 1:
                test = line.split()[1].strip()
                if 'MOVE' in test and not '_START' in test and not '_MOVES_H' in test and not 'NUM_OF' in test:
                    moves_dict[test] = index
                    index += 1
    return moves_dict


def load_machine_move_list(file_path):
    move_list = []
    in_array = False
    move_pattern = re.compile(r'\bMOVE_[A-Z0-9_]+')

    with open(file_path, encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if 'static const u16 sMachineMoves[]' in line:
                in_array = True
                continue
            if in_array:
                if '};' in line:
                    break
                matches = move_pattern.findall(line)
                move_list.extend(matches)

    return move_list


def load_form_to_species_mapping(form_map_path):
    form_map = {}

    define_pattern = re.compile(r"\[(SPECIES_\w+)\s*-\s*SPECIES_MEGA_START\]\s*=\s*(SPECIES_\w+),")

    with open(form_map_path) as f:
        for line in f:
            match = define_pattern.search(line)
            if match:
                form_species, base_species = match.groups()
                form_map[form_species] = base_species

    return form_map


def merge_learnsets(ordered_data, cutoff_gen, inherit_level, inherit_egg, inherit_machine, inherit_tutor):
    merged = {}

    for gen_file, gen_data in ordered_data:
        gen_key = os.path.basename(gen_file)[:-5]  # remove .json extension
        if re.match(r"^\d{2}_", gen_key):
            gen_key = gen_key[3:]  # remove XX_ prefix

        for species, fields in gen_data.items():
            merged.setdefault(species, {
                "LevelMoves": [],
                "MachineMoves": set(),
                "EggMoves": [],
                "TutorMoves": set()
            })

            if fields.get("LevelMoves") and (gen_key == cutoff_gen or inherit_level):
                merged[species]["LevelMoves"] = fields.get("LevelMoves", [])

            if fields.get("EggMoves") and (gen_key == cutoff_gen or inherit_egg):
                merged[species]["EggMoves"] = fields.get("EggMoves", [])

            if gen_key == cutoff_gen or inherit_machine:
                merged[species]["MachineMoves"].update(fields.get("MachineMoves", []))

            if gen_key == cutoff_gen or inherit_tutor:
                merged[species]["TutorMoves"].update(fields.get("TutorMoves", []))

        if gen_key == cutoff_gen:
            break

    for data in merged.values():
        data["MachineMoves"] = sorted(data["MachineMoves"])
        data["TutorMoves"] = sorted(data["TutorMoves"])

    return merged


def load_tutor_move_list(file_path):
    move_list = []
    in_array = False
    move_pattern = re.compile(r'\bMOVE_[A-Z0-9_]+')

    with open(file_path, encoding='utf-8') as f:
        for line in f:
            stripped = line.strip()
            if stripped.startswith("TutorMove sTutorMoves[]"):
                in_array = True
                continue
            if in_array:
                if stripped.startswith("};"):
                    break
                matches = move_pattern.findall(stripped)
                if matches:
                    move_list.append(matches[0])
    return move_list


def write_learnset_constants_inc(max_num_levelup_moves, output_path):
    os.makedirs(os.path.dirname(output_path), exist_ok=True)

    with open(output_path, "w") as f:
        f.write("// DO NOT MODIFY THIS FILE! Autogenerated by build_learnsets.py\n\n")
        f.write(f"MAX_LEVELUP_MOVES equ {max_num_levelup_moves}")


def write_learnset_constants_header(num_machine_moves, max_num_levelup_moves, max_num_egg_moves, num_tutor_moves, output_path):
    os.makedirs(os.path.dirname(output_path), exist_ok=True)

    with open(output_path, "w") as f:
        f.write("// DO NOT MODIFY THIS FILE! Autogenerated by build_learnsets.py\n\n")
        f.write("#ifndef GENERATED_LEARNSET_CONSTANTS_H\n")
        f.write("#define GENERATED_LEARNSET_CONSTANTS_H\n\n")
        f.write(f"#define NUM_MACHINE_MOVES {num_machine_moves}\n")
        f.write(f"#define MAX_LEVELUP_MOVES {max_num_levelup_moves}\n")
        f.write(f"#define MAX_EGG_MOVES     {max_num_egg_moves}\n")
        f.write(f"#define NUM_TUTOR_MOVES   {num_tutor_moves}\n\n")
        f.write(f"#define MACHINE_LEARNSETS_BITFIELD_COUNT ((NUM_MACHINE_MOVES + 31) / 32)\n")
        f.write(f"#define TUTOR_LEARNSETS_BITFIELD_COUNT   ((NUM_TUTOR_MOVES + 31) / 32)\n\n")
        f.write("#endif // GENERATED_LEARNSET_CONSTANTS_H\n")


def write_machine_data(species_dict, species_learnsets, machine_moves, output_path):
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    max_species_index = max(species_dict.values())
    species_id_to_name = {v: k for k, v in species_dict.items()}

    with open(output_path, "w") as out:
        out.write("// DO NOT MODIFY THIS FILE!  autogenerated by build_learnsets.py\n\n")
        out.write("#include \"../../include/types.h\"\n")
        out.write("#include \"../../include/constants/species.h\"\n")
        out.write("#include \"../../include/constants/generated/learnsets.h\"\n\n")
        out.write(f"const u32 UNUSED MachineMoveLearnsets[][MACHINE_LEARNSETS_BITFIELD_COUNT] = {{\n")

        for species_id in range(max_species_index + 1):
            species_name = species_id_to_name.get(species_id)
            learnset = []
            levelup_moves = {}
            if species_name:
                learnset = species_learnsets.get(species_name, {}).get("MachineMoves", [])
                learnset = list(set(m.strip() for m in learnset))

                levelup_moves = {
                    m["Move"] for m in species_learnsets.get(species_name, {}).get("LevelMoves", [])
                    if "Move" in m
                }

            parts = [0] * ((len(machine_moves) + 31) // 32)
            for i, move in enumerate(machine_moves):
                if move in learnset or move in levelup_moves:
                    move_index = i
                else:
                    continue

                word = move_index // 32
                bit = move_index % 32
                parts[word] |= (1 << bit)

            formatted = ", ".join(f"0x{val:08X}" for val in parts)
            out.write(f"    [{species_name}] = {{ {formatted} }},\n")

        out.write("};\n")


def write_levelup_data(species_dict, moves_dict, species_learnsets, max_num_levelup_moves, output_path):
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    species_id_to_name = {v: k for k, v in species_dict.items()}
    max_species_id = max(species_id_to_name.keys())
    col_len = 8

    with open(output_path, "w") as out:
        out.write("// DO NOT MODIFY THIS FILE! Autogenerated by build_learnsets.py\n\n")
        out.write("#include \"../../include/types.h\"\n")
        out.write("#include \"../../include/constants/generated/learnsets.h\"\n")
        out.write("#include \"../../include/constants/species.h\"\n\n")
        out.write("const u32 UNUSED LevelUpLearnsets[][MAX_LEVELUP_MOVES] = {\n")

        for species_id in range(max_species_id + 1):
            species_name = species_id_to_name.get(species_id)
            learnset = []
            if species_name:
                learnset = species_learnsets.get(species_name, {}).get("LevelMoves", [])

            entries = []

            for move_entry in learnset:
                move = move_entry.get("Move", "").strip()
                level = int(move_entry["Level"])
                if not move or move not in moves_dict:
                    print(f"[ERROR]: Invalid or missing move '{move}' for species '{species_name}' at level {level}")
                    exit(1)

                move_id = moves_dict[move]
                encoded = (level << 16) | move_id
                entries.append(encoded)

            entries.append(0x0000FFFF)
            while len(entries) < max_num_levelup_moves:
                entries.append(0x0000FFFF)

            out.write(f"    [{species_name}] = {{\n")
            for i in range(0, max_num_levelup_moves, col_len):
                line = ", ".join(f"0x{val:08X}" for val in entries[i:i+col_len])
                out.write(f"        {line},\n")
            out.write("    },\n")

        out.write("};\n")


def write_eggmove_data(species_dict, moves_dict, species_learnsets, max_num_egg_moves, output_path):
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    species_id_to_name = {v: k for k, v in species_dict.items()}
    max_species_id = max(species_id_to_name)
    col_len = 12

    with open(output_path, "w") as out:
        out.write("// DO NOT MODIFY THIS FILE! Autogenerated by build_learnsets.py\n\n")
        out.write("#include \"../../include/types.h\"\n")
        out.write("#include \"../../include/constants/generated/learnsets.h\"\n")
        out.write("#include \"../../include/constants/species.h\"\n\n")
        out.write("const u16 UNUSED EggMoves[][MAX_EGG_MOVES] = {\n")

        for species_id in range(max_species_id + 1):
            species_name = species_id_to_name.get(species_id, "")
            egg_moves = []

            if species_name:
                egg_moves = species_learnsets.get(species_name, {}).get("EggMoves", [])

            moves = []
            for move in egg_moves:
                if move not in moves_dict:
                    print(f"[ERROR]: Move '{move}' not found in moves.h")
                    exit(1)
                moves.append(moves_dict[move])

            # Add terminator and pad to fixed length
            moves.append(0xFFFF)
            while len(moves) < max_num_egg_moves:
                moves.append(0xFFFF)

            out.write(f"    [{species_name}] = {{\n")
            for i in range(0, max_num_egg_moves, col_len):
                chunk = moves[i:i+col_len]
                out.write("        " + ", ".join(f"0x{m:04X}" for m in chunk) + ",\n")
            out.write("    },\n")

        out.write("};\n")


def write_tutor_data(species_dict, moves_dict, species_learnsets, tutor_moves, output_path):
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    move_to_index = {
        move_name: idx
        for idx, move_name in enumerate(tutor_moves)
        if move_name in moves_dict
    }

    max_species_index = max(species_dict.values()) if species_dict else 0
    species_id_to_name = {v: k for k, v in species_dict.items()}

    words_per_row = (len(tutor_moves) + 31) // 32

    with open(output_path, "w", encoding="utf-8") as out:
        out.write("// DO NOT MODIFY THIS FILE!  autogenerated by build_learnsets.py\n\n")
        out.write("#include \"../../include/types.h\"\n")
        out.write("#include \"../../include/constants/species.h\"\n")
        out.write("#include \"../../include/constants/generated/learnsets.h\"\n\n")
        out.write("const u32 UNUSED TutorLearnsets[][TUTOR_LEARNSETS_BITFIELD_COUNT] = {\n")

        for species_id in range(max_species_index + 1):
            species_name = species_id_to_name.get(species_id)

            parts = [0] * words_per_row
            if species_name:
                tutor_list = species_learnsets.get(species_name, {}).get("TutorMoves", [])
                for move in tutor_list:
                    idx = move_to_index.get(move)
                    if idx is None:
                        continue
                    word = idx // 32
                    bit = idx % 32
                    parts[word] |= (1 << bit)

            formatted = ", ".join(f"0x{val:08X}" for val in parts)
            out.write(f"    [{species_name if species_name else 'SPECIES_NONE'}] = {{ {formatted} }},\n")

        out.write("};\n")


if __name__ == "__main__":
    parser = argparse.ArgumentParser()

    # build
    parser.add_argument("--learnsets")
    parser.add_argument("--machineout")
    parser.add_argument("--levelupout")
    parser.add_argument("--eggout")
    parser.add_argument("--tutorout")
    parser.add_argument("--constsout", action='store_true')
    parser.add_argument("--dump")

    # generate
    parser.add_argument("--generate")
    parser.add_argument("--cutoff", default="sv")
    parser.add_argument("--inherit-level", action="store_true")
    parser.add_argument("--inherit-egg", action="store_true")
    parser.add_argument("--inherit-machine", action="store_true")
    parser.add_argument("--inherit-tutor", action="store_true")

    args = parser.parse_args()

    machine_moves = load_machine_move_list("src/item.c")
    tutor_moves = load_tutor_move_list("src/field/move_tutor.c")
    species_dict = load_species_header("include/constants/species.h")
    moves_dict = load_moves_header("include/constants/moves.h")

    if args.generate:
        ordered_learnsets = [
            (file, json.load(open(file, encoding="utf-8")))
            for file in sorted(glob.glob(os.path.join("data/learnsets/base", "*.json")))
        ]

        merged_for_dump = merge_learnsets(
            ordered_learnsets,
            args.cutoff,
            args.inherit_level,
            args.inherit_egg,
            args.inherit_machine,
            args.inherit_tutor,
        )

        os.makedirs(os.path.dirname(args.generate), exist_ok=True)
        with open(args.generate, "w", encoding="utf-8") as f:
            json.dump(merged_for_dump, f, indent=2)

    if any([args.machineout, args.levelupout, args.eggout, args.tutorout, args.constsout]):
        form_to_base = load_form_to_species_mapping("data/FormToSpeciesMapping.c")
        
        if not args.learnsets:
            print(f"[ERROR]: {msg}")
            exit(1)

        try:
            with open(args.learnsets, "r", encoding="utf-8") as f:
                species_learnsets = json.load(f)
        except FileNotFoundError:
            print(f"[ERROR]: learnsets file not found: {args.learnsets}")
            exit(1)

        for form_species, base_species in form_to_base.items():
            if form_species not in species_learnsets and base_species in species_learnsets:
                species_learnsets[form_species] = dict(species_learnsets[base_species])

        max_num_levelup_moves = max(
            (len(data.get("LevelMoves", [])) + 1)  # +1 for terminator
            for data in species_learnsets.values()
        )

        if max_num_levelup_moves > (256 / 4):
            print(f"[ERROR]: maximum number of level-up moves cannot exceed 64 ({max_num_levelup_moves})")
            exit(1)

        max_num_egg_moves = max(
            (len(data.get("EggMoves", [])) + 1)  # +1 for terminator
            for data in species_learnsets.values()
        )

        if args.constsout:
            write_learnset_constants_header(
                len(machine_moves),
                max_num_levelup_moves,
                max_num_egg_moves,
                len(tutor_moves),
                "include/constants/generated/learnsets.h",
            )
            write_learnset_constants_inc(max_num_levelup_moves, "armips/include/generated/levelup.s")

        if args.machineout:
            write_machine_data(species_dict, species_learnsets, machine_moves, args.machineout)

        if args.levelupout:
            write_levelup_data(species_dict, moves_dict, species_learnsets, max_num_levelup_moves, args.levelupout)

        if args.eggout:
            write_eggmove_data(species_dict, moves_dict, species_learnsets, max_num_egg_moves, args.eggout)

        if args.tutorout:
            write_tutor_data(species_dict, moves_dict, species_learnsets, tutor_moves, args.tutorout)
        
        if args.dump:
            os.makedirs(os.path.dirname(args.dump), exist_ok=True)
            with open(args.dump, "w", encoding="utf-8") as f:
                json.dump(species_learnsets, f, indent=2)